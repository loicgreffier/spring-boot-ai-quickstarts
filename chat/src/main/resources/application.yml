spring:
  ai:
    ollama:
      base-url: "http://localhost:11434"
      chat:
        options:
          model: "llama3.2:3b"
          temperature: 0.7
      init:
        pull-model-strategy: "WHEN_MISSING"
        timeout: "5m"
        max-retries: 1
